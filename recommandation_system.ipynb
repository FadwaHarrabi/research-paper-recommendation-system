{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Graph neural networks (GNNs) have been widely ...  \n",
       "1  Deep networks and decision forests (such as ra...  \n",
       "2  Graph convolutional networks (GCNs) are powerf...  \n",
       "3  With the increasing popularity of Graph Neural...  \n",
       "4  Machine learning solutions for pattern classif...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"arxiv_data_210930-054931.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56181 entries, 0 to 56180\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   terms      56181 non-null  object\n",
      " 1   titles     56181 non-null  object\n",
      " 2   abstracts  56181 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cs.LG', 'cs.AI', 'cs.CR', ...,\n",
       "       'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3',\n",
       "       '68T07, 68T45, 68T10, 68T50, 68U35', 'I.2.0; G.3'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_column=df['terms'].apply(literal_eval)\n",
    "label=labels_column.explode().unique()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['titles'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41105, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2503"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"terms\"].value_counts()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter=df.groupby('terms').filter(lambda x: len(x)> 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38602, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter[\"terms\"]=df_filter[\"terms\"].apply(lambda x:literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter[\"terms\"].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=train_test_split(df_filter,test_size=0.2,stratify=df_filter[\"terms\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30881, 3), (7721, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df=test_df.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(val_df.index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30881, 3), (3861, 3), (3860, 3))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape,test_df.shape,val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=tf.ragged.constant(train_df[\"terms\"])\n",
    "lookup=tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(terms)\n",
    "vocab=lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=150\n",
    "batch_size=128\n",
    "padding_token=\"<pad>\"\n",
    "auto=tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b'Synthetic images are one of the most promising solutions to avoid high costs\\nassociated with generating annotated datasets to train supervised convolutional\\nneural networks (CNN). However, to allow networks to generalize knowledge from\\nsynthetic to real images, domain adaptation methods are necessary. This paper\\nimplements unsupervised domain adaptation (UDA) methods on an anchorless object\\ndetector. Given their good performance, anchorless detectors are increasingly\\nattracting attention in the field of object detection. While their results are\\ncomparable to the well-established anchor-based methods, anchorless detectors\\nare considerably faster. In our work, we use CenterNet, one of the most recent\\nanchorless architectures, for a domain adaptation problem involving synthetic\\nimages. Taking advantage of the architecture of anchorless detectors, we\\npropose to adjust two UDA methods, viz., entropy minimization and maximum\\nsquares loss, originally developed for segmentation, to object detection. Our\\nresults show that the proposed UDA methods can increase the mAPfrom61 %to69\\n%with respect to direct transfer on the considered anchorless detector. The\\ncode is available: https://github.com/scheckmedia/centernet-uda.'\n",
      " \n",
      "Abstract: b'We consider the problem of image representation for the tasks of unsupervised\\nlearning and semi-supervised learning. In those learning tasks, the raw image\\nvectors may not provide enough representation for their intrinsic structures\\ndue to their highly dense feature space. To overcome this problem, the raw\\nimage vectors should be mapped to a proper representation space which can\\ncapture the latent structure of the original data and represent the data\\nexplicitly for further learning tasks such as clustering.\\n  Inspired by the recent research works on deep neural network and\\nrepresentation learning, in this paper, we introduce the multiple-layer\\nauto-encoder into image representation, we also apply the locally invariant\\nideal to our image representation with auto-encoders and propose a novel\\nmethod, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact\\nrepresentation which uncovers the hidden semantics and simultaneously respects\\nthe intrinsic geometric structure.\\n  Extensive experiments on image clustering show encouraging results of the\\nproposed algorithm in comparison to the state-of-the-art algorithms on\\nreal-word cases.'\n",
      " \n",
      "Abstract: b'Deep Neural Networks(DNNs) require huge GPU memory when training on modern\\nimage/video databases. Unfortunately, the GPU memory is physically finite,\\nwhich limits the image resolutions and batch sizes that could be used in\\ntraining for better DNN performance. Unlike solutions that require physically\\nupgrade GPUs, the Gradient CheckPointing(GCP) training trades computation for\\nmore memory beyond existing GPU hardware. GCP only stores a subset of\\nintermediate tensors, called Gradient Checkpoints (GCs), during forward. Then\\nduring backward, extra local forwards are conducted to compute the missing\\ntensors. The total training memory cost becomes the sum of (1) the memory cost\\nof the gradient checkpoints and (2) the maximum memory cost of local forwards.\\nTo achieve maximal memory cut-offs, one needs optimal algorithms to select GCs.\\nExisting GCP approaches rely on either manual input of GCs or heuristics-based\\nGC search on Linear Computation Graphs (LCGs), and cannot apply to Arbitrary\\nComputation Graphs(ACGs). In this paper, we present theories and optimal\\nalgorithms on GC selection that, for the first time, are applicable to ACGs and\\nachieve the maximal memory cut-offs. Extensive experiments show that our\\napproach not only outperforms existing approaches (only applicable on LCGs),\\nand is applicable to a vast family of LCG and ACG networks, such as Alexnet,\\nVGG, ResNet, Densenet, Inception Net and highly complicated DNNs by Network\\nArchitecture Search. Our work enables GCP training on ACGs, and cuts off up-to\\n80% of training memory with a moderate time overhead (~30%-50%). Codes are\\navailable'\n",
      " \n",
      "Abstract: b'In this work, we focus on a challenging task: synthesizing multiple imaginary\\nvideos given a single image. Major problems come from high dimensionality of\\npixel space and the ambiguity of potential motions. To overcome those problems,\\nwe propose a new framework that produce imaginary videos by transformation\\ngeneration. The generated transformations are applied to the original image in\\na novel volumetric merge network to reconstruct frames in imaginary video.\\nThrough sampling different latent variables, our method can output different\\nimaginary video samples. The framework is trained in an adversarial way with\\nunsupervised learning. For evaluation, we propose a new assessment metric\\n$RIQA$. In experiments, we test on 3 datasets varying from synthetic data to\\nnatural scene. Our framework achieves promising performance in image quality\\nassessment. The visual inspection indicates that it can successfully generate\\ndiverse five-frame videos in acceptable perceptual quality.'\n",
      " \n",
      "Abstract: b'Intelligent agents need to generalize from past experience to achieve goals\\nin complex environments. World models facilitate such generalization and allow\\nlearning behaviors from imagined outcomes to increase sample-efficiency. While\\nlearning world models from image inputs has recently become feasible for some\\ntasks, modeling Atari games accurately enough to derive successful behaviors\\nhas remained an open challenge for many years. We introduce DreamerV2, a\\nreinforcement learning agent that learns behaviors purely from predictions in\\nthe compact latent space of a powerful world model. The world model uses\\ndiscrete representations and is trained separately from the policy. DreamerV2\\nconstitutes the first agent that achieves human-level performance on the Atari\\nbenchmark of 55 tasks by learning behaviors inside a separately trained world\\nmodel. With the same computational budget and wall-clock time, Dreamer V2\\nreaches 200M frames and surpasses the final performance of the top single-GPU\\nagents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous\\nactions, where it learns an accurate world model of a complex humanoid robot\\nand solves stand-up and walking from only pixel inputs.'\n",
      " \n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    # print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146951\n"
     ]
    }
   ],
   "source": [
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommandation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"terms\",\"abstracts\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56181 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles\n",
       "0      Multi-Level Attention Pooling for Graph Neural...\n",
       "1      Decision Forests vs. Deep Networks: Conceptual...\n",
       "2      Power up! Robust Graph Convolutional Network v...\n",
       "3      Releasing Graph Neural Networks with Different...\n",
       "4      Recurrence-Aware Long-Term Cognitive Network f...\n",
       "...                                                  ...\n",
       "56176  Mining Spatio-temporal Data on Industrializati...\n",
       "56177  Wav2Letter: an End-to-End ConvNet-based Speech...\n",
       "56178  Deep Reinforcement Learning with Double Q-lear...\n",
       "56179                        Generalized Low Rank Models\n",
       "56180  Chi-square Tests Driven Method for Learning th...\n",
       "\n",
       "[56181 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\pfa\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\pfa\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model=SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=data['titles']\n",
    "embeddings=model.encode(sentences)  #convert text data into numerical representations (embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "embedding 384\n",
      "\n",
      "sentence Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "embedding 384\n",
      "\n",
      "sentence Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "embedding 384\n",
      "\n",
      "sentence Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "embedding 384\n",
      "\n",
      "sentence Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "embedding 384\n",
      "\n",
      "sentence Lifelong Graph Learning\n",
      "embedding 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for sentence,embedding in zip(sentences,embeddings):\n",
    "    print(\"sentence\",sentence)\n",
    "    print(\"embedding\",len(embedding))\n",
    "    print(\"\")\n",
    "    if c >= 5:\n",
    "        break\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why select **all-MiniLM-L6-v2** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings.pkl','wb') as f:\n",
    "    pickle.dump(embeddings,f)\n",
    "\n",
    "with open('sentences.pkl','wb') as f:\n",
    "    pickle.dump(sentences,f)\n",
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recommendation for similar papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "embeddings=pickle.load(open(\"models/embeddings.pkl\",'rb'))\n",
    "sentences=pickle.load(open(\"models/sentences.pkl\",\"rb\"))\n",
    "model=pickle.load(open(\"models/model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def recommendation(input_paper):\n",
    "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
    "    cosine_scores = util.cos_sim(embeddings, model.encode(input_paper))\n",
    "    \n",
    "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
    "    top_similar_papers = torch.topk(cosine_scores, dim=0, k=5, sorted=True)\n",
    "                                 \n",
    "    # Retrieve the titles of the top similar papers.\n",
    "    papers_list = []\n",
    "    for i in top_similar_papers.indices:\n",
    "        papers_list.append(sentences[i.item()])\n",
    "    \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Persuasive Faces: Generating Faces in Advertisements',\n",
       " 'Copy this Sentence',\n",
       " 'Area Attention',\n",
       " 'Area Attention',\n",
       " '\"You eat with your eyes first\": Optimizing Yelp Image Advertising']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_paper=input('attention')\n",
    "recommandation_papers=recommendation(input_paper)\n",
    "recommandation_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n",
      "2.3.1\n",
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "# install this versions\n",
    "import sentence_transformers\n",
    "import tensorflow\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
